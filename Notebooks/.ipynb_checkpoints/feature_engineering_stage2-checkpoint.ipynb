{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa65b38-8366-449e-8350-b870262ae786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df =pd.read_csv(\"../Data/clean_data/encoded_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fd84f-aed0-41ea-aa6b-c41d0ab04c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one from each highly correlated pair\n",
    "# Why: loan_amount_log and person_income_log preserve relationships while handling skew\n",
    "# Low-variance one-hot features add noise without predictive power\n",
    "df.drop(columns=['loan_amount', 'person_income'], inplace=True)  # Keeping log versions\n",
    "\n",
    "# Drop low-variance one-hot columns (threshold <5%)\n",
    "low_variance_cols = ['person_home_ownership_other', 'loan_purpose_venture']\n",
    "df.drop(columns=low_variance_cols, inplace=True)\n",
    "\n",
    "# Verify\n",
    "print(f\"Columns remaining: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ee7d3-7c27-4176-8107-9652a1c0018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Why These Features:\n",
    "\n",
    "### risk_tier: Captures how interest rates compound with loan grade risk\n",
    "\n",
    "### income_adequacy: More sensitive to low-income borrowers\n",
    "\n",
    "### employment_stability: Non-linear relationship with default risk\n",
    "\n",
    "### debt_burden: Penalizes large loans more severely\n",
    "# 1. Risk Tier (combines grade and interest rate)\n",
    "df['risk_tier'] = df['loan_grade_numeric'] * df['loan_interest_rate']\n",
    "\n",
    "# 2. Income Adequacy Ratio (more sensitive than standard ratio)\n",
    "df['income_adequacy'] = df['person_income_log'] / (df['loan_amount_log'] + 1) \n",
    "\n",
    "# 3. Employment Stability Bins\n",
    "df['employment_stability'] = pd.cut(\n",
    "    df['person_employment_length'],\n",
    "    bins=[0, 2, 5, 10, np.inf],\n",
    "    labels=['new', 'mid', 'experienced', 'veteran']\n",
    ")\n",
    "\n",
    "# 4. Debt Burden Interaction\n",
    "df['debt_burden'] = np.sqrt(df['loan_amount_log']) * df['loan_to_income_ratio']\n",
    "\n",
    "# Drop original columns we replaced\n",
    "df.drop(columns=['loan_grade_numeric', 'person_employment_length'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11dcbd-b18d-421d-825b-b7a56c2d01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the new stability bins\n",
    "df = pd.get_dummies(df, columns=['employment_stability'], prefix='emp')\n",
    "\n",
    "# Ordinal encode credit_history_bins (assuming ordered categories)\n",
    "bin_order = ['<1y', '1-3y', '3-5y', '5-10y', '10+']\n",
    "df['credit_history_ordinal'] = df['credit_history_bins'].map(\n",
    "    {bin: i for i, bin in enumerate(bin_order)}\n",
    ")\n",
    "df.drop(columns=['credit_history_bins'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09e35a-2d22-4e42-8d83-69437e029e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1215f8f-0ab8-4029-94c6-dedc8b878cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('loan_status', axis=1)\n",
    "y = df['loan_status']\n",
    "\n",
    "# Stratified split preserves class ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Default rate - Train: {y_train.mean():.2%}, Test: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60584d82-797c-4bbb-9956-f4d40e5193e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Manual Oversampling\n",
    "def oversample(X, y, target_minority_pct=0.5):\n",
    "    \"\"\"\n",
    "    target_minority_pct: Desired minority class percentage (e.g., 0.5 = 50%)\n",
    "    \"\"\"\n",
    "    n_majority = sum(y == 0)\n",
    "    n_minority = sum(y == 1)\n",
    "    target_minority = int(n_majority * target_minority_pct / (1 - target_minority_pct))\n",
    "    needed = target_minority - n_minority\n",
    "    \n",
    "    if needed <= 0:\n",
    "        print(\"Already balanced or minority class exceeds target percentage.\")\n",
    "        return X, y\n",
    "    \n",
    "    X_minority = X[y == 1]\n",
    "    X_extra = X_minority.sample(needed, replace=True, random_state=42)\n",
    "    y_extra = pd.Series(1, index=X_extra.index)\n",
    "    \n",
    "    return pd.concat([X, X_extra]), pd.concat([y, y_extra])\n",
    "\n",
    "# Apply with 50% minority class\n",
    "X_train_res, y_train_res = oversample(X_train, y_train, target_minority_pct=0.5)\n",
    "print(f\"New class balance: {y_train_res.value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14985156-2f7d-463f-8c2e-0e3b7248c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "# --- After oversampling ---\n",
    "\n",
    "# Encode loan_grade column (important!)\n",
    "grade_order = [['A', 'B', 'C', 'D', 'E_or_lower']]\n",
    "encoder = OrdinalEncoder(categories=grade_order)\n",
    "\n",
    "X_train_res['loan_grade'] = encoder.fit_transform(X_train_res[['loan_grade']])\n",
    "X_test['loan_grade'] = encoder.transform(X_test[['loan_grade']])\n",
    "\n",
    "# Now scale numeric features\n",
    "num_cols = X_train_res.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "binary_cols = [col for col in num_cols if X_train_res[col].nunique() == 2]\n",
    "num_cols = [col for col in num_cols if col not in binary_cols]\n",
    "num_cols = [col for col in num_cols if X_train_res[col].nunique(dropna=True) > 1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_res[num_cols] = scaler.fit_transform(X_train_res[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4474a6-3079-43f0-b0e4-19e3e83e7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    class_weight=None,  # No need for class_weight because of oversampling\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_res_pred = model.predict(X_train_res)\n",
    "y_train_res_proba = model.predict_proba(X_train_res)[:, 1]\n",
    "\n",
    "print(\"Train Classification Report:\\n\", classification_report(y_train_res, y_train_res_pred))\n",
    "print(\"Train ROC AUC:\", roc_auc_score(y_train_res, y_train_res_proba))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_test_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284a7f6-ef58-48e4-bab3-4c8676d804ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Initialize results dictionary (if not already initialized)\n",
    "results = {}\n",
    "\n",
    "# Define LightGBM model\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    class_weight='balanced',  # optional with manual oversampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on manually resampled training data\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict on original test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Predict on resampled training data\n",
    "y_train_res_pred = model.predict(X_train_res)\n",
    "y_train_res_proba = model.predict_proba(X_train_res)[:, 1]\n",
    "\n",
    "# Store evaluation results\n",
    "results[\"LightGBM\"] = {\n",
    "    \"test_report\": classification_report(y_test, y_test_pred),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_test_proba),\n",
    "    \"train_resampled_report\": classification_report(y_train_res, y_train_res_pred),\n",
    "    \"train_resampled_roc_auc\": roc_auc_score(y_train_res, y_train_res_proba)\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nLightGBM Model Performance (After Manual Oversampling):\")\n",
    "print(\"TRAIN SET (Resampled):\")\n",
    "print(results[\"LightGBM\"][\"train_resampled_report\"])\n",
    "print(f\"ROC-AUC: {results['LightGBM']['train_resampled_roc_auc']:.4f}\")\n",
    "print(\"\\nTEST SET (Original):\")\n",
    "print(results[\"LightGBM\"][\"test_report\"])\n",
    "print(f\"ROC-AUC: {results['LightGBM']['test_roc_auc']:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd6fea-e8ee-4108-8569-110076ea8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Get predicted probabilities for the positive class (1)\n",
    "y_scores = model.predict_proba(X_test_imp)[:, 1]\n",
    "\n",
    "# Calculate precision, recall, thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Calculate F1 scores for each threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)  # avoid div by zero\n",
    "\n",
    "# Find threshold with max F1 score\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Best threshold based on F1 score: {best_threshold:.4f}\")\n",
    "print(f\"Best F1 score at this threshold: {best_f1:.4f}\")\n",
    "\n",
    "# Predict with the best threshold\n",
    "y_pred_best_thresh = (y_scores >= best_threshold).astype(int)\n",
    "\n",
    "# Show classification report with best threshold\n",
    "print(\"\\nClassification report with best threshold:\")\n",
    "print(classification_report(y_test, y_pred_best_thresh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3143f29-bdc3-45f4-8618-d4011ab5a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def find_threshold_for_min_recall(y_true, y_scores, target_recall=0.8):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    # recall and precision length = len(thresholds) + 1\n",
    "    # find indices where recall >= target_recall\n",
    "    valid_indices = [i for i, r in enumerate(recall[:-1]) if r >= target_recall]\n",
    "    if not valid_indices:\n",
    "        print(\"Warning: Cannot achieve target recall\")\n",
    "        return 0.5  # fallback\n",
    "    \n",
    "    # Choose threshold corresponding to the highest threshold among those indices\n",
    "    best_idx = valid_indices[-1]  # last index where recall >= target_recall\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "threshold_recall = find_threshold_for_min_recall(y_test, y_test_proba, 0.8)\n",
    "print(f\"Threshold for recall >= 0.8: {threshold_recall:.4f}\")\n",
    "\n",
    "y_pred_recall = (y_test_proba >= threshold_recall).astype(int)\n",
    "print(classification_report(y_test, y_pred_recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0052da-bbb8-4e66-beeb-02c5864bb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# List of important features to keep (exact names as in your data)\n",
    "important_features = [\n",
    "    'person_income_log',\n",
    "    'loan_interest_rate',\n",
    "    'income_adequacy',\n",
    "    'person_age',\n",
    "    'loan_amount_log',\n",
    "    'risk_tier',\n",
    "    'debt_burden'\n",
    "]\n",
    "\n",
    "# Select only important features from training and test sets\n",
    "X_train_res_imp = X_train_res[important_features]\n",
    "X_test_imp = X_test[important_features]\n",
    "\n",
    "# Initialize results dictionary (if not already initialized)\n",
    "results = {}\n",
    "\n",
    "# Define LightGBM model\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    class_weight='balanced',  # optional if you used manual oversampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on manually resampled training data (with important features only)\n",
    "model.fit(X_train_res_imp, y_train_res)\n",
    "\n",
    "# Predict on original test set (important features only)\n",
    "y_test_pred = model.predict(X_test_imp)\n",
    "y_test_proba = model.predict_proba(X_test_imp)[:, 1]\n",
    "\n",
    "# Predict on resampled training data (important features only)\n",
    "y_train_res_pred = model.predict(X_train_res_imp)\n",
    "y_train_res_proba = model.predict_proba(X_train_res_imp)[:, 1]\n",
    "\n",
    "# Store evaluation results\n",
    "results[\"LightGBM\"] = {\n",
    "    \"test_report\": classification_report(y_test, y_test_pred),\n",
    "    \"test_roc_auc\": roc_auc_score(y_test, y_test_proba),\n",
    "    \"train_resampled_report\": classification_report(y_train_res, y_train_res_pred),\n",
    "    \"train_resampled_roc_auc\": roc_auc_score(y_train_res, y_train_res_proba)\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nLightGBM Model Performance (After Manual Oversampling) - Important Features Only:\")\n",
    "print(\"TRAIN SET (Resampled):\")\n",
    "print(results[\"LightGBM\"][\"train_resampled_report\"])\n",
    "print(f\"ROC-AUC: {results['LightGBM']['train_resampled_roc_auc']:.4f}\")\n",
    "print(\"\\nTEST SET (Original):\")\n",
    "print(results[\"LightGBM\"][\"test_report\"])\n",
    "print(f\"ROC-AUC: {results['LightGBM']['test_roc_auc']:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfc3d8-c0cc-4f0b-88d2-4e7f6c08b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# After fitting your LGBM model:\n",
    "lgbm_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feat_imp_lgbm = pd.DataFrame({\n",
    "    'feature': X_train_res.columns,\n",
    "    'importance': lgbm_importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_imp_lgbm['feature'], feat_imp_lgbm['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance - LightGBM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f2e6e-279e-4b32-939f-ce2179f83a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_lgbm['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4596eda-4fc4-4bb9-bd7a-cde424581247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    roc_auc_score, \n",
    "    precision_recall_curve,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# =============================================\n",
    "# 1. Define Threshold Finder Function\n",
    "# =============================================\n",
    "def find_optimal_threshold(y_true, y_probs, target_recall):\n",
    "    \"\"\"Find threshold achieving at least target recall\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    try:\n",
    "        best_idx = np.where(recall[:-1] >= target_recall)[0][-1]\n",
    "        return thresholds[best_idx]\n",
    "    except IndexError:\n",
    "        max_recall = recall.max()\n",
    "        print(f\"Warning: Cannot achieve {target_recall:.0%} recall. Max recall is {max_recall:.2%}\")\n",
    "        return thresholds[np.argmax(recall[:-1])]\n",
    "\n",
    "# =============================================\n",
    "# 2. Initialize and Train Model\n",
    "# =============================================\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    class_weight={0:1, 1:3},  # heavier weight for minority class\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on resampled training data\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# =============================================\n",
    "# 3. Threshold Tuning on Test Set\n",
    "# =============================================\n",
    "y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "optimal_threshold = find_optimal_threshold(y_test, y_test_probs, 0.90)\n",
    "\n",
    "print(f\"\\nOptimal threshold for 90% recall: {optimal_threshold:.4f}\")\n",
    "\n",
    "y_pred_high_recall = (y_test_probs >= optimal_threshold).astype(int)\n",
    "achieved_recall = recall_score(y_test, y_pred_high_recall, pos_label=1)\n",
    "print(f\"Achieved recall at optimal threshold: {achieved_recall:.2%}\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Grid Search with Recall Optimization\n",
    "# =============================================\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'class_weight': [{0:1, 1:3}, {0:1, 1:5}]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lgb.LGBMClassifier(n_estimators=200, random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=recall_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train_res, y_train_res)\n",
    "\n",
    "# =============================================\n",
    "# 5. Evaluation Helper Function\n",
    "# =============================================\n",
    "def evaluate_model(name, model, X, y, threshold=0.5):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(classification_report(y, preds))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y, probs):.4f}\")\n",
    "    print(f\"Default Rate in Predictions: {preds.mean():.2%}\")\n",
    "    return preds\n",
    "\n",
    "# Evaluate initial model with tuned threshold\n",
    "print(\"\\n=== Initial Model with Threshold Tuning ===\")\n",
    "evaluate_model(\"Tuned Model\", model, X_test, y_test, optimal_threshold)\n",
    "\n",
    "# Evaluate best model from GridSearch\n",
    "print(\"\\n=== Best GridSearch Model ===\")\n",
    "best_model = grid.best_estimator_\n",
    "evaluate_model(\"GridSearch Best\", best_model, X_test, y_test)\n",
    "\n",
    "# =============================================\n",
    "# 6. Threshold Comparison on initial model\n",
    "# =============================================\n",
    "print(\"\\nThreshold Comparison on Initial Model:\")\n",
    "for threshold in [0.3, 0.5, optimal_threshold]:\n",
    "    preds = (y_test_probs >= threshold).astype(int)\n",
    "    rec = recall_score(y_test, preds, pos_label=1)\n",
    "    prec = precision_score(y_test, preds, pos_label=1)\n",
    "    print(f\"Threshold {threshold:.2f}: Recall={rec:.2f}, Precision={prec:.2f}\")\n",
    "\n",
    "# =============================================\n",
    "# 7. Final Model Selection\n",
    "# =============================================\n",
    "# Choose final model here:\n",
    "# final_model = model            # initial tuned model\n",
    "final_model = best_model         # grid search best\n",
    "\n",
    "print(\"\\nFinal model selected:\", final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97aea9-75ec-4f31-84b1-37c7657fadac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
